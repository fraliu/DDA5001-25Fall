{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-30T06:44:15.726938Z",
     "iopub.status.busy": "2025-11-30T06:44:15.726691Z",
     "iopub.status.idle": "2025-11-30T06:46:08.677986Z",
     "shell.execute_reply": "2025-11-30T06:46:08.677265Z",
     "shell.execute_reply.started": "2025-11-30T06:44:15.726913Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "dask-cuda 25.2.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
      "cuml-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
      "cudf-cu12 25.2.2 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
      "distributed-ucxx-cu12 0.42.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
      "google-adk 1.18.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.26.0 which is incompatible.\n",
      "google-adk 1.18.0 requires opentelemetry-exporter-otlp-proto-http>=1.36.0, but you have opentelemetry-exporter-otlp-proto-http 1.26.0 which is incompatible.\n",
      "google-adk 1.18.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.26.0 which is incompatible.\n",
      "ydata-profiling 4.17.0 requires numba<=0.61,>=0.56.0, but you have numba 0.61.2 which is incompatible.\n",
      "a2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 4.25.7 which is incompatible.\n",
      "opentelemetry-resourcedetector-gcp 1.11.0a0 requires opentelemetry-api~=1.30, but you have opentelemetry-api 1.26.0 which is incompatible.\n",
      "opentelemetry-resourcedetector-gcp 1.11.0a0 requires opentelemetry-sdk~=1.30, but you have opentelemetry-sdk 1.26.0 which is incompatible.\n",
      "opentelemetry-exporter-gcp-monitoring 1.11.0a0 requires opentelemetry-api~=1.30, but you have opentelemetry-api 1.26.0 which is incompatible.\n",
      "opentelemetry-exporter-gcp-monitoring 1.11.0a0 requires opentelemetry-sdk~=1.30, but you have opentelemetry-sdk 1.26.0 which is incompatible.\n",
      "opentelemetry-exporter-gcp-trace 1.11.0 requires opentelemetry-api~=1.30, but you have opentelemetry-api 1.26.0 which is incompatible.\n",
      "opentelemetry-exporter-gcp-trace 1.11.0 requires opentelemetry-sdk~=1.30, but you have opentelemetry-sdk 1.26.0 which is incompatible.\n",
      "preprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.2 which is incompatible.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-api>=1.35.0, but you have opentelemetry-api 1.26.0 which is incompatible.\n",
      "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.26.0 which is incompatible.\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\n",
      "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\n",
      "pydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\n",
      "pydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\n",
      "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.7 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\n",
      "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.2.2 which is incompatible.\n",
      "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.7 which is incompatible.\n",
      "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.9/207.9 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for pylatexenc (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install --no-index --find-links=/kaggle/input/it-is-vllm-0-8-5 -q vllm\n",
    "!pip install -q pylatexenc math-verify[antlr4_9_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T06:56:08.102344Z",
     "iopub.status.busy": "2025-11-30T06:56:08.101600Z",
     "iopub.status.idle": "2025-11-30T06:56:08.489626Z",
     "shell.execute_reply": "2025-11-30T06:56:08.488944Z",
     "shell.execute_reply.started": "2025-11-30T06:56:08.102307Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/kaggle/working/src’: File exists\n",
      "/kaggle/working/src\n",
      "inference.py  evaluate.py  outputs\tverifier\n"
     ]
    }
   ],
   "source": [
    "!mkdir /kaggle/working/src\n",
    "!cp -r /kaggle/input/evaluation/* /kaggle/working/src\n",
    "\n",
    "%cd /kaggle/working/src\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T06:56:08.491326Z",
     "iopub.status.busy": "2025-11-30T06:56:08.491094Z",
     "iopub.status.idle": "2025-11-30T07:26:53.265846Z",
     "shell.execute_reply": "2025-11-30T07:26:53.265064Z",
     "shell.execute_reply.started": "2025-11-30T06:56:08.491303Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-30 06:56:18.562237: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764485778.775410     110 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764485778.828759     110 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "INFO 11-30 06:56:35 [__init__.py:239] Automatically detected platform cuda.\n",
      "DP Rank 0 (Local 0) mapped to GPU(s): 0\n",
      "DP Rank 1 (Local 1) mapped to GPU(s): 1\n",
      "2025-11-30 06:56:44.043115: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-11-30 06:56:44.043161: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764485804.062590     131 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764485804.062594     132 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764485804.068672     132 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "E0000 00:00:1764485804.068920     131 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "INFO 11-30 06:56:50 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 11-30 06:56:50 [__init__.py:239] Automatically detected platform cuda.\n",
      "[Rank 0] Logical GPU count: 1\n",
      "[Rank 0] Loading dataset: math -> math-ai/math500\n",
      "[Rank 1] Logical GPU count: 1\n",
      "[Rank 1] Loading dataset: math -> math-ai/math500\n",
      "README.md: 100%|███████████████████████████████| 412/412 [00:00<00:00, 3.23MB/s]\n",
      "test.jsonl: 447kB [00:00, 23.0MB/s]\n",
      "Generating test split: 100%|████████| 500/500 [00:00<00:00, 18450.94 examples/s]\n",
      "tokenizer_config.json: 7.32kB [00:00, 31.9MB/s]\n",
      "vocab.json: 2.78MB [00:00, 74.7MB/s]\n",
      "merges.txt: 1.67MB [00:00, 132MB/s]\n",
      "tokenizer.json: 7.03MB [00:00, 187MB/s]\n",
      "[Rank 1] Needs to process 250 prompts (indices [250, 500))\n",
      "[Rank 0] Needs to process 250 prompts (indices [0, 250))\n",
      "config.json: 100%|█████████████████████████████| 656/656 [00:00<00:00, 5.97MB/s]\n",
      "WARNING 11-30 06:57:01 [config.py:2972] Casting torch.bfloat16 to torch.float16.\n",
      "WARNING 11-30 06:57:01 [config.py:2972] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 11-30 06:57:26 [config.py:717] This model supports multiple tasks: {'classify', 'generate', 'score', 'reward', 'embed'}. Defaulting to 'generate'.\n",
      "WARNING 11-30 06:57:26 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \n",
      "INFO 11-30 06:57:26 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5) with config: model='Qwen/Qwen2.5-Math-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-Math-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=3072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-Math-1.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 11-30 06:57:26 [config.py:717] This model supports multiple tasks: {'embed', 'generate', 'score', 'reward', 'classify'}. Defaulting to 'generate'.\n",
      "WARNING 11-30 06:57:26 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \n",
      "INFO 11-30 06:57:26 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5) with config: model='Qwen/Qwen2.5-Math-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-Math-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=3072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-Math-1.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "generation_config.json: 100%|██████████████████| 160/160 [00:00<00:00, 1.33MB/s]\n",
      "INFO 11-30 06:57:27 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 11-30 06:57:27 [cuda.py:289] Using XFormers backend.\n",
      "INFO 11-30 06:57:27 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 11-30 06:57:27 [cuda.py:289] Using XFormers backend.\n",
      "[W1130 06:57:28.620282719 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W1130 06:57:28.620622042 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W1130 06:57:28.620990570 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W1130 06:57:28.621358520 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "INFO 11-30 06:57:28 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 11-30 06:57:28 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 11-30 06:57:28 [model_runner.py:1108] Starting to load model Qwen/Qwen2.5-Math-1.5B-Instruct...\n",
      "INFO 11-30 06:57:28 [model_runner.py:1108] Starting to load model Qwen/Qwen2.5-Math-1.5B-Instruct...\n",
      "INFO 11-30 06:57:29 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "INFO 11-30 06:57:29 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "model.safetensors: 100%|████████████████████| 3.09G/3.09G [00:07<00:00, 410MB/s]\n",
      "INFO 11-30 06:57:36 [weight_utils.py:281] Time spent downloading weights for Qwen/Qwen2.5-Math-1.5B-Instruct: 7.730346 seconds\n",
      "INFO 11-30 06:57:36 [weight_utils.py:315] No model.safetensors.index.json found in remote.\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "INFO 11-30 06:57:37 [weight_utils.py:315] No model.safetensors.index.json found in remote.\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.56s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.56s/it]\n",
      "\n",
      "INFO 11-30 06:57:40 [loader.py:458] Loading weights took 3.62 seconds\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.49s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.49s/it]\n",
      "\n",
      "INFO 11-30 06:57:40 [loader.py:458] Loading weights took 3.68 seconds\n",
      "INFO 11-30 06:57:41 [model_runner.py:1140] Model loading took 2.8798 GiB and 11.758888 seconds\n",
      "INFO 11-30 06:57:41 [model_runner.py:1140] Model loading took 2.8798 GiB and 11.971052 seconds\n",
      "INFO 11-30 06:57:42 [worker.py:287] Memory profiling takes 1.24 seconds\n",
      "INFO 11-30 06:57:42 [worker.py:287] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.90) = 13.27GiB\n",
      "INFO 11-30 06:57:42 [worker.py:287] model weights take 2.88GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 8.95GiB.\n",
      "INFO 11-30 06:57:42 [worker.py:287] Memory profiling takes 1.12 seconds\n",
      "INFO 11-30 06:57:42 [worker.py:287] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.90) = 13.27GiB\n",
      "INFO 11-30 06:57:42 [worker.py:287] model weights take 2.88GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 8.95GiB.\n",
      "INFO 11-30 06:57:43 [executor_base.py:112] # cuda blocks: 20939, # CPU blocks: 9362\n",
      "INFO 11-30 06:57:43 [executor_base.py:117] Maximum concurrency for 3072 tokens per request: 109.06x\n",
      "INFO 11-30 06:57:43 [executor_base.py:112] # cuda blocks: 20939, # CPU blocks: 9362\n",
      "INFO 11-30 06:57:43 [executor_base.py:117] Maximum concurrency for 3072 tokens per request: 109.06x\n",
      "INFO 11-30 06:57:48 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "Capturing CUDA graph shapes:   0%|                       | 0/35 [00:00<?, ?it/s]INFO 11-30 06:57:48 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "Capturing CUDA graph shapes: 100%|██████████████| 35/35 [00:33<00:00,  1.05it/s]\n",
      "INFO 11-30 06:58:22 [model_runner.py:1592] Graph capturing finished in 33 secs, took 0.19 GiB\n",
      "INFO 11-30 06:58:22 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 41.27 seconds\n",
      "Capturing CUDA graph shapes: 100%|██████████████| 35/35 [00:33<00:00,  1.05it/s]\n",
      "INFO 11-30 06:58:22 [model_runner.py:1592] Graph capturing finished in 33 secs, took 0.19 GiB\n",
      "INFO 11-30 06:58:22 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 41.10 seconds\n",
      "[Rank 1] Starting generation for 250 prompts with batch_size=16\n",
      "[Rank 0] Starting generation for 250 prompts with batch_size=16\n",
      "Processed prompts: 100%|█| 256/256 [01:04<00:00,  3.99it/s, est. speed input: 33\n",
      "[Rank 1] Processed 16/250 prompts with n=16 completions each\n",
      "Processed prompts: 100%|█| 256/256 [01:31<00:00,  2.80it/s, est. speed input: 33\n",
      "[Rank 0] Processed 16/250 prompts with n=16 completions each\n",
      "Processed prompts: 100%|█| 256/256 [01:29<00:00,  2.85it/s, est. speed input: 29\n",
      "[Rank 1] Processed 32/250 prompts with n=16 completions each\n",
      "Processed prompts: 100%|█| 256/256 [01:35<00:00,  2.69it/s, est. speed input: 25\n",
      "[Rank 0] Processed 32/250 prompts with n=16 completions each\n",
      "Processed prompts: 100%|█| 256/256 [01:29<00:00,  2.87it/s, est. speed input: 22\n",
      "[Rank 0] Processed 48/250 prompts with n=16 completions each\n",
      "Processed prompts: 100%|█| 256/256 [02:21<00:00,  1.80it/s, est. speed input: 20\n",
      "[Rank 1] Processed 48/250 prompts with n=16 completions each\n",
      "Processed prompts: 100%|█| 256/256 [01:35<00:00,  2.67it/s, est. speed input: 19\n",
      "[Rank 0] Processed 64/250 prompts with n=16 completions each\n",
      "Processed prompts: 100%|█| 256/256 [01:52<00:00,  2.28it/s, est. speed input: 29\n",
      "[Rank 1] Processed 64/250 prompts with n=16 completions each\n",
      "Processed prompts: 100%|█| 256/256 [01:17<00:00,  3.30it/s, est. speed input: 22\n",
      "[Rank 0] Processed 80/250 prompts with n=16 completions each\n",
      "Processed prompts: 100%|█| 256/256 [01:44<00:00,  2.45it/s, est. speed input: 23\n",
      "[Rank 1] Processed 80/250 prompts with n=16 completions each\n",
      "Processed prompts: 100%|█| 256/256 [01:36<00:00,  2.64it/s, est. speed input: 25\n",
      "[Rank 0] Processed 96/250 prompts with n=16 completions each\n",
      "Processed prompts: 100%|█| 256/256 [01:49<00:00,  2.34it/s, est. speed input: 16\n",
      "[Rank 1] Processed 96/250 prompts with n=16 completions each\n",
      "Processed prompts: 100%|█| 256/256 [02:19<00:00,  1.84it/s, est. speed input: 24\n",
      "[Rank 0] Processed 112/250 prompts with n=16 completions each\n",
      "Processed prompts: 100%|█| 256/256 [01:46<00:00,  2.41it/s, est. speed input: 23\n",
      "[Rank 1] Processed 112/250 prompts with n=16 completions each\n",
      "Processed prompts: 100%|█| 256/256 [01:51<00:00,  2.29it/s, est. speed input: 20\n",
      "[Rank 0] Processed 128/250 prompts with n=16 completions each\n",
      "Processed prompts: 100%|█| 256/256 [01:31<00:00,  2.80it/s, est. speed input: 27\n",
      "[Rank 1] Processed 128/250 prompts with n=16 completions each\n",
      "Processed prompts: 100%|█| 256/256 [01:36<00:00,  2.65it/s, est. speed input: 22\n",
      "[Rank 0] Processed 144/250 prompts with n=16 completions each\n",
      "Processed prompts: 100%|█| 256/256 [01:58<00:00,  2.17it/s, est. speed input: 24\n",
      "[Rank 1] Processed 144/250 prompts with n=16 completions each\n",
      "Processed prompts: 100%|█| 256/256 [01:59<00:00,  2.15it/s, est. speed input: 24\n",
      "[Rank 0] Processed 160/250 prompts with n=16 completions each\n",
      "Processed prompts: 100%|█| 256/256 [01:55<00:00,  2.21it/s, est. speed input: 19\n",
      "[Rank 1] Processed 160/250 prompts with n=16 completions each\n",
      "Processed prompts: 100%|█| 256/256 [01:52<00:00,  2.28it/s, est. speed input: 27\n",
      "[Rank 0] Processed 176/250 prompts with n=16 completions each\n",
      "Processed prompts: 100%|█| 256/256 [02:08<00:00,  1.99it/s, est. speed input: 17\n",
      "[Rank 1] Processed 176/250 prompts with n=16 completions each\n",
      "Processed prompts: 100%|█| 256/256 [01:35<00:00,  2.69it/s, est. speed input: 32\n",
      "[Rank 0] Processed 192/250 prompts with n=16 completions each\n",
      "Processed prompts: 100%|█| 256/256 [01:54<00:00,  2.24it/s, est. speed input: 17\n",
      "[Rank 1] Processed 192/250 prompts with n=16 completions each\n",
      "Processed prompts: 100%|█| 256/256 [01:25<00:00,  2.99it/s, est. speed input: 25\n",
      "[Rank 0] Processed 208/250 prompts with n=16 completions each\n",
      "Processed prompts: 100%|█| 256/256 [01:24<00:00,  3.04it/s, est. speed input: 39\n",
      "[Rank 0] Processed 224/250 prompts with n=16 completions each\n",
      "Processed prompts: 100%|█| 256/256 [01:43<00:00,  2.48it/s, est. speed input: 23\n",
      "[Rank 1] Processed 208/250 prompts with n=16 completions each\n",
      "Processed prompts: 100%|█| 256/256 [01:34<00:00,  2.71it/s, est. speed input: 30\n",
      "[Rank 1] Processed 224/250 prompts with n=16 completions each\n",
      "Processed prompts: 100%|█| 256/256 [02:01<00:00,  2.11it/s, est. speed input: 19\n",
      "[Rank 0] Processed 240/250 prompts with n=16 completions each\n",
      "Processed prompts: 100%|█| 160/160 [01:34<00:00,  1.69it/s, est. speed input: 16\n",
      "[Rank 0] Processed 250/250 prompts with n=16 completions each\n",
      "[Rank 0] Saved partial JSONL to outputs/math500.rank0.jsonl\n",
      "[rank0]:[W1130 07:25:12.086257526 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "Processed prompts: 100%|█| 256/256 [02:09<00:00,  1.97it/s, est. speed input: 19\n",
      "[Rank 1] Processed 240/250 prompts with n=16 completions each\n",
      "Processed prompts: 100%|█| 160/160 [01:19<00:00,  2.01it/s, est. speed input: 19\n",
      "[Rank 1] Processed 250/250 prompts with n=16 completions each\n",
      "[Rank 1] Saved partial JSONL to outputs/math500.rank1.jsonl\n",
      "[rank0]:[W1130 07:26:48.321952219 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "Total time consuming is: 1810.98s\n",
      "Merged 2 partial files into /kaggle/working/src/outputs/math500.jsonl\n"
     ]
    }
   ],
   "source": [
    "!python inference.py \\\n",
    "  --model \"Qwen/Qwen2.5-Math-1.5B-Instruct\" \\\n",
    "  --dataset \"math\" \\\n",
    "  --dp-size 2 \\\n",
    "  --batch-size 16 \\\n",
    "  --rollout-n 16 \\\n",
    "  --temperature 1.0 \\\n",
    "  --top-p 0.9 \\\n",
    "  --output_file outputs/math500.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T07:27:15.874650Z",
     "iopub.status.busy": "2025-11-30T07:27:15.873816Z",
     "iopub.status.idle": "2025-11-30T07:27:43.575482Z",
     "shell.execute_reply": "2025-11-30T07:27:43.574767Z",
     "shell.execute_reply.started": "2025-11-30T07:27:15.874619Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting lines in outputs/math500.jsonl...\n",
      "Scoring generations from outputs/math500.jsonl...\n",
      "Processing lines: 100%|████████████████████| 8000/8000 [00:26<00:00, 300.22it/s]\n",
      "Processing complete. Scored 8000 lines across 500 unique problems.\n",
      "\n",
      "Pass@k Metrics:\n",
      "  pass@1   : 74.51%\n",
      "  pass@2   : 81.00%\n",
      "  pass@4   : 86.06%\n",
      "  pass@8   : 89.98%\n",
      "  pass@16  : 92.80%\n",
      "\n",
      "Majority Vote Metric:\n",
      "  maj@1    : 79.20%\n",
      "\n",
      "Scored results saved to outputs/math500_eval.jsonl\n"
     ]
    }
   ],
   "source": [
    "!python evaluate.py --input_file outputs/math500.jsonl --output_file outputs/math500_eval.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T07:29:56.445772Z",
     "iopub.status.busy": "2025-11-30T07:29:56.444874Z",
     "iopub.status.idle": "2025-11-30T07:43:25.546282Z",
     "shell.execute_reply": "2025-11-30T07:43:25.545346Z",
     "shell.execute_reply.started": "2025-11-30T07:29:56.445729Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-30 07:30:00.807682: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764487800.827730     354 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764487800.833840     354 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "INFO 11-30 07:30:07 [__init__.py:239] Automatically detected platform cuda.\n",
      "DP Rank 0 (Local 0) mapped to GPU(s): 0\n",
      "DP Rank 1 (Local 1) mapped to GPU(s): 1\n",
      "2025-11-30 07:30:14.831388: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764487814.851792     373 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764487814.857697     373 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-11-30 07:30:14.875328: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764487814.896694     374 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764487814.903962     374 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "INFO 11-30 07:30:21 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 11-30 07:30:21 [__init__.py:239] Automatically detected platform cuda.\n",
      "[Rank 0] Logical GPU count: 1\n",
      "[Rank 0] Loading dataset: amc -> math-ai/amc23\n",
      "[Rank 1] Logical GPU count: 1\n",
      "[Rank 1] Loading dataset: amc -> math-ai/amc23\n",
      "README.md: 100%|███████████████████████████████| 374/374 [00:00<00:00, 2.28MB/s]\n",
      "test-00000-of-00001.parquet: 100%|█████████| 11.9k/11.9k [00:00<00:00, 40.7kB/s]\n",
      "Generating test split: 100%|███████████| 40/40 [00:00<00:00, 1052.90 examples/s]\n",
      "[Rank 1] Needs to process 20 prompts (indices [20, 40))\n",
      "[Rank 0] Needs to process 20 prompts (indices [0, 20))\n",
      "WARNING 11-30 07:30:31 [config.py:2972] Casting torch.bfloat16 to torch.float16.\n",
      "WARNING 11-30 07:30:31 [config.py:2972] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 11-30 07:30:55 [config.py:717] This model supports multiple tasks: {'embed', 'score', 'reward', 'generate', 'classify'}. Defaulting to 'generate'.\n",
      "WARNING 11-30 07:30:55 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \n",
      "INFO 11-30 07:30:55 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5) with config: model='Qwen/Qwen2.5-Math-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-Math-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=3072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-Math-1.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 11-30 07:30:55 [config.py:717] This model supports multiple tasks: {'classify', 'embed', 'generate', 'reward', 'score'}. Defaulting to 'generate'.\n",
      "WARNING 11-30 07:30:55 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \n",
      "INFO 11-30 07:30:55 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5) with config: model='Qwen/Qwen2.5-Math-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-Math-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=3072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-Math-1.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 11-30 07:30:56 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 11-30 07:30:56 [cuda.py:289] Using XFormers backend.\n",
      "INFO 11-30 07:30:57 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 11-30 07:30:57 [cuda.py:289] Using XFormers backend.\n",
      "[W1130 07:30:57.647143229 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W1130 07:30:57.647908754 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "INFO 11-30 07:30:57 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 11-30 07:30:57 [model_runner.py:1108] Starting to load model Qwen/Qwen2.5-Math-1.5B-Instruct...\n",
      "[W1130 07:30:57.686314074 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W1130 07:30:57.687070608 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "INFO 11-30 07:30:57 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 11-30 07:30:57 [model_runner.py:1108] Starting to load model Qwen/Qwen2.5-Math-1.5B-Instruct...\n",
      "INFO 11-30 07:30:58 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "INFO 11-30 07:30:58 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "INFO 11-30 07:30:58 [weight_utils.py:315] No model.safetensors.index.json found in remote.\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "INFO 11-30 07:30:58 [weight_utils.py:315] No model.safetensors.index.json found in remote.\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.18s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.18s/it]\n",
      "\n",
      "INFO 11-30 07:31:01 [loader.py:458] Loading weights took 3.24 seconds\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.25s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.25s/it]\n",
      "\n",
      "INFO 11-30 07:31:01 [loader.py:458] Loading weights took 3.32 seconds\n",
      "INFO 11-30 07:31:01 [model_runner.py:1140] Model loading took 2.8798 GiB and 3.585358 seconds\n",
      "INFO 11-30 07:31:02 [model_runner.py:1140] Model loading took 2.8798 GiB and 3.768569 seconds\n",
      "INFO 11-30 07:31:03 [worker.py:287] Memory profiling takes 1.23 seconds\n",
      "INFO 11-30 07:31:03 [worker.py:287] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.90) = 13.27GiB\n",
      "INFO 11-30 07:31:03 [worker.py:287] model weights take 2.88GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 8.95GiB.\n",
      "INFO 11-30 07:31:03 [worker.py:287] Memory profiling takes 1.18 seconds\n",
      "INFO 11-30 07:31:03 [worker.py:287] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.90) = 13.27GiB\n",
      "INFO 11-30 07:31:03 [worker.py:287] model weights take 2.88GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 8.95GiB.\n",
      "INFO 11-30 07:31:03 [executor_base.py:112] # cuda blocks: 20939, # CPU blocks: 9362\n",
      "INFO 11-30 07:31:03 [executor_base.py:117] Maximum concurrency for 3072 tokens per request: 109.06x\n",
      "INFO 11-30 07:31:04 [executor_base.py:112] # cuda blocks: 20939, # CPU blocks: 9362\n",
      "INFO 11-30 07:31:04 [executor_base.py:117] Maximum concurrency for 3072 tokens per request: 109.06x\n",
      "INFO 11-30 07:31:08 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "Capturing CUDA graph shapes:   0%|                       | 0/35 [00:00<?, ?it/s]INFO 11-30 07:31:08 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "Capturing CUDA graph shapes: 100%|██████████████| 35/35 [00:32<00:00,  1.07it/s]\n",
      "INFO 11-30 07:31:40 [model_runner.py:1592] Graph capturing finished in 33 secs, took 0.19 GiB\n",
      "INFO 11-30 07:31:40 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 39.03 seconds\n",
      "[Rank 0] Starting generation for 20 prompts with batch_size=16\n",
      "Capturing CUDA graph shapes: 100%|██████████████| 35/35 [00:33<00:00,  1.06it/s]\n",
      "INFO 11-30 07:31:41 [model_runner.py:1592] Graph capturing finished in 33 secs, took 0.19 GiB\n",
      "INFO 11-30 07:31:41 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 39.37 seconds\n",
      "[Rank 1] Starting generation for 20 prompts with batch_size=16\n",
      "Processed prompts: 100%|█| 1024/1024 [08:41<00:00,  1.97it/s, est. speed input: \n",
      "[Rank 0] Processed 16/20 prompts with n=64 completions each\n",
      "Processed prompts: 100%|█| 1024/1024 [08:54<00:00,  1.92it/s, est. speed input: \n",
      "[Rank 1] Processed 16/20 prompts with n=64 completions each\n",
      "Processed prompts: 100%|█| 256/256 [02:07<00:00,  2.01it/s, est. speed input: 17\n",
      "[Rank 0] Processed 20/20 prompts with n=64 completions each\n",
      "[Rank 0] Saved partial JSONL to outputs/amc23.rank0.jsonl\n",
      "[rank0]:[W1130 07:42:32.025602494 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "Processed prompts: 100%|█| 256/256 [02:41<00:00,  1.58it/s, est. speed input: 14\n",
      "[Rank 1] Processed 20/20 prompts with n=64 completions each\n",
      "[Rank 1] Saved partial JSONL to outputs/amc23.rank1.jsonl\n",
      "[rank0]:[W1130 07:43:20.738419448 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "Total time consuming is: 792.54s\n",
      "Merged 2 partial files into /kaggle/working/src/outputs/amc23.jsonl\n"
     ]
    }
   ],
   "source": [
    "!python inference.py \\\n",
    "  --model \"Qwen/Qwen2.5-Math-1.5B-Instruct\" \\\n",
    "  --dataset \"amc\" \\\n",
    "  --dp-size 2 \\\n",
    "  --batch-size 16 \\\n",
    "  --rollout-n 64 \\\n",
    "  --temperature 1.0 \\\n",
    "  --top-p 0.9 \\\n",
    "  --output_file outputs/amc23.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T07:43:25.548495Z",
     "iopub.status.busy": "2025-11-30T07:43:25.547860Z",
     "iopub.status.idle": "2025-11-30T07:43:32.445760Z",
     "shell.execute_reply": "2025-11-30T07:43:32.445107Z",
     "shell.execute_reply.started": "2025-11-30T07:43:25.548462Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting lines in outputs/amc23.jsonl...\n",
      "Scoring generations from outputs/amc23.jsonl...\n",
      "Processing lines: 100%|████████████████████| 2560/2560 [00:05<00:00, 428.98it/s]\n",
      "Processing complete. Scored 2560 lines across 40 unique problems.\n",
      "\n",
      "Pass@k Metrics:\n",
      "  pass@1   : 53.20%\n",
      "  pass@2   : 64.23%\n",
      "  pass@4   : 73.23%\n",
      "  pass@8   : 80.29%\n",
      "  pass@16  : 85.97%\n",
      "  pass@32  : 90.04%\n",
      "  pass@64  : 92.50%\n",
      "\n",
      "Majority Vote Metric:\n",
      "  maj@1    : 62.50%\n",
      "\n",
      "Scored results saved to outputs/amc23_eval.jsonl\n"
     ]
    }
   ],
   "source": [
    "!python evaluate.py --input_file outputs/amc23.jsonl --output_file outputs/amc23_eval.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T07:46:52.601912Z",
     "iopub.status.busy": "2025-11-30T07:46:52.601244Z",
     "iopub.status.idle": "2025-11-30T07:58:58.144127Z",
     "shell.execute_reply": "2025-11-30T07:58:58.143238Z",
     "shell.execute_reply.started": "2025-11-30T07:46:52.601861Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-30 07:46:56.475763: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764488816.495723     545 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764488816.501953     545 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "INFO 11-30 07:47:02 [__init__.py:239] Automatically detected platform cuda.\n",
      "DP Rank 0 (Local 0) mapped to GPU(s): 0\n",
      "DP Rank 1 (Local 1) mapped to GPU(s): 1\n",
      "2025-11-30 07:47:10.207401: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-11-30 07:47:10.208077: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764488830.228275     564 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764488830.228543     565 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764488830.234283     564 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "E0000 00:00:1764488830.234621     565 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "INFO 11-30 07:47:16 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 11-30 07:47:16 [__init__.py:239] Automatically detected platform cuda.\n",
      "[Rank 1] Logical GPU count: 1\n",
      "[Rank 1] Loading dataset: aime -> math-ai/aime25\n",
      "[Rank 0] Logical GPU count: 1\n",
      "[Rank 0] Loading dataset: aime -> math-ai/aime25\n",
      "README.md: 100%|███████████████████████████████| 753/753 [00:00<00:00, 4.36MB/s]\n",
      "test.jsonl: 15.8kB [00:00, 45.8MB/s]\n",
      "Generating test split: 100%|███████████| 30/30 [00:00<00:00, 8828.26 examples/s]\n",
      "[Rank 1] Needs to process 15 prompts (indices [15, 30))\n",
      "[Rank 0] Needs to process 15 prompts (indices [0, 15))\n",
      "WARNING 11-30 07:47:26 [config.py:2972] Casting torch.bfloat16 to torch.float16.\n",
      "WARNING 11-30 07:47:26 [config.py:2972] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 11-30 07:47:50 [config.py:717] This model supports multiple tasks: {'embed', 'classify', 'score', 'reward', 'generate'}. Defaulting to 'generate'.\n",
      "WARNING 11-30 07:47:50 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \n",
      "INFO 11-30 07:47:50 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5) with config: model='Qwen/Qwen2.5-Math-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-Math-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=3072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-Math-1.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 11-30 07:47:50 [config.py:717] This model supports multiple tasks: {'embed', 'reward', 'classify', 'score', 'generate'}. Defaulting to 'generate'.\n",
      "WARNING 11-30 07:47:50 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \n",
      "INFO 11-30 07:47:50 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5) with config: model='Qwen/Qwen2.5-Math-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-Math-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=3072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-Math-1.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 11-30 07:47:52 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 11-30 07:47:52 [cuda.py:289] Using XFormers backend.\n",
      "INFO 11-30 07:47:52 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 11-30 07:47:52 [cuda.py:289] Using XFormers backend.\n",
      "[W1130 07:47:52.694957796 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W1130 07:47:52.695691043 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W1130 07:47:52.699636432 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W1130 07:47:52.700260119 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "INFO 11-30 07:47:52 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 11-30 07:47:52 [model_runner.py:1108] Starting to load model Qwen/Qwen2.5-Math-1.5B-Instruct...\n",
      "INFO 11-30 07:47:52 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 11-30 07:47:52 [model_runner.py:1108] Starting to load model Qwen/Qwen2.5-Math-1.5B-Instruct...\n",
      "INFO 11-30 07:47:53 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "INFO 11-30 07:47:53 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "INFO 11-30 07:47:53 [weight_utils.py:315] No model.safetensors.index.json found in remote.\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "INFO 11-30 07:47:53 [weight_utils.py:315] No model.safetensors.index.json found in remote.\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.25s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.25s/it]\n",
      "\n",
      "INFO 11-30 07:47:56 [loader.py:458] Loading weights took 3.32 seconds\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.31s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.31s/it]\n",
      "\n",
      "INFO 11-30 07:47:56 [loader.py:458] Loading weights took 3.38 seconds\n",
      "INFO 11-30 07:47:57 [model_runner.py:1140] Model loading took 2.8798 GiB and 3.718111 seconds\n",
      "INFO 11-30 07:47:57 [model_runner.py:1140] Model loading took 2.8798 GiB and 3.906736 seconds\n",
      "INFO 11-30 07:47:58 [worker.py:287] Memory profiling takes 1.14 seconds\n",
      "INFO 11-30 07:47:58 [worker.py:287] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.90) = 13.27GiB\n",
      "INFO 11-30 07:47:58 [worker.py:287] model weights take 2.88GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 8.95GiB.\n",
      "INFO 11-30 07:47:58 [worker.py:287] Memory profiling takes 1.17 seconds\n",
      "INFO 11-30 07:47:58 [worker.py:287] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.90) = 13.27GiB\n",
      "INFO 11-30 07:47:58 [worker.py:287] model weights take 2.88GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 8.95GiB.\n",
      "INFO 11-30 07:47:59 [executor_base.py:112] # cuda blocks: 20939, # CPU blocks: 9362\n",
      "INFO 11-30 07:47:59 [executor_base.py:117] Maximum concurrency for 3072 tokens per request: 109.06x\n",
      "INFO 11-30 07:47:59 [executor_base.py:112] # cuda blocks: 20939, # CPU blocks: 9362\n",
      "INFO 11-30 07:47:59 [executor_base.py:117] Maximum concurrency for 3072 tokens per request: 109.06x\n",
      "INFO 11-30 07:48:03 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "Capturing CUDA graph shapes:   0%|                       | 0/35 [00:00<?, ?it/s]INFO 11-30 07:48:03 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "Capturing CUDA graph shapes: 100%|██████████████| 35/35 [00:33<00:00,  1.05it/s]\n",
      "INFO 11-30 07:48:36 [model_runner.py:1592] Graph capturing finished in 33 secs, took 0.19 GiB\n",
      "INFO 11-30 07:48:36 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 39.63 seconds\n",
      "Capturing CUDA graph shapes: 100%|██████████████| 35/35 [00:33<00:00,  1.05it/s]\n",
      "INFO 11-30 07:48:36 [model_runner.py:1592] Graph capturing finished in 33 secs, took 0.19 GiB\n",
      "INFO 11-30 07:48:36 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 39.57 seconds\n",
      "[Rank 0] Starting generation for 15 prompts with batch_size=16\n",
      "[Rank 1] Starting generation for 15 prompts with batch_size=16\n",
      "Processed prompts: 100%|█| 960/960 [09:48<00:00,  1.63it/s, est. speed input: 32\n",
      "[Rank 0] Processed 15/15 prompts with n=64 completions each\n",
      "[Rank 0] Saved partial JSONL to outputs/aime25.rank0.jsonl\n",
      "[rank0]:[W1130 07:58:27.675924984 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "Processed prompts: 100%|█| 960/960 [10:13<00:00,  1.56it/s, est. speed input: 31\n",
      "[Rank 1] Processed 15/15 prompts with n=64 completions each\n",
      "[Rank 1] Saved partial JSONL to outputs/aime25.rank1.jsonl\n",
      "[rank0]:[W1130 07:58:53.276048810 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "Total time consuming is: 709.66s\n",
      "Merged 2 partial files into /kaggle/working/src/outputs/aime25.jsonl\n"
     ]
    }
   ],
   "source": [
    "!python inference.py \\\n",
    "  --model \"Qwen/Qwen2.5-Math-1.5B-Instruct\" \\\n",
    "  --dataset \"aime\" \\\n",
    "  --dp-size 2 \\\n",
    "  --batch-size 16 \\\n",
    "  --rollout-n 64 \\\n",
    "  --temperature 1.0 \\\n",
    "  --top-p 0.9 \\\n",
    "  --output_file outputs/aime25.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T07:58:58.145807Z",
     "iopub.status.busy": "2025-11-30T07:58:58.145571Z",
     "iopub.status.idle": "2025-11-30T07:59:07.047868Z",
     "shell.execute_reply": "2025-11-30T07:59:07.047117Z",
     "shell.execute_reply.started": "2025-11-30T07:58:58.145784Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting lines in outputs/aime25.jsonl...\n",
      "Scoring generations from outputs/aime25.jsonl...\n",
      "Processing lines:  83%|████████████████▌   | 1594/1920 [00:04<00:00, 330.44it/s]Timeout during comparison\n",
      "Processing lines:  85%|█████████████████▊   | 1628/1920 [00:06<00:03, 80.76it/s]Timeout during comparison\n",
      "Processing lines: 100%|████████████████████| 1920/1920 [00:07<00:00, 240.73it/s]\n",
      "Processing complete. Scored 1920 lines across 30 unique problems.\n",
      "\n",
      "Pass@k Metrics:\n",
      "  pass@1   : 8.07%\n",
      "  pass@2   : 13.54%\n",
      "  pass@4   : 20.31%\n",
      "  pass@8   : 27.31%\n",
      "  pass@16  : 34.40%\n",
      "  pass@32  : 42.77%\n",
      "  pass@64  : 53.33%\n",
      "\n",
      "Majority Vote Metric:\n",
      "  maj@1    : 20.00%\n",
      "\n",
      "Scored results saved to outputs/aime25_eval.jsonl\n"
     ]
    }
   ],
   "source": [
    "!python evaluate.py --input_file outputs/aime25.jsonl --output_file outputs/aime25_eval.jsonl"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7425745,
     "sourceId": 11821657,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8831591,
     "sourceId": 13927941,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
